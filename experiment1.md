





\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\boldU}{\mathbf{U}}
\newcommand{\boldW}{\mathbf{W}}
\newcommand{\boldV}{\mathbf{V}}
\newcommand{\boldX}{\mathbf{X}}
\newcommand{\boldQ}{\mathbf{Q}}
\newcommand{\boldI}{\mathbf{I}}
\newcommand{\boldbeta}{\boldsymbol{\beta}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

# MLFS method

Suppose we have $N$ data points, each having a label $y_i \in \{1, ..., C\}$ and information from various views $\boldX^{(m)}, m \in \{1, ..., M\}$. 

For each view $m$, there are view-specific latent matrices $\boldU^{(m)} \in \R^{N \times d_m}$ which are generated by a *shared* latent representation $\boldV \in \R^{N \times R}$ and view-specific sparse weight matrices $\boldW^{(m)} \in \R^{R \times d_m}$. Each row $\boldU_i^{(m)}$ is generated as follows

\[
\boldU_i^{(m)} \sim \N(\boldV_i \boldW^{(m)}, \frac{1}{\gamma_m} \boldI)
\]

Our data $\boldX_{ij}^{(m)}$ is then generated

- for gaussian views, $\boldX_{ij}^{(m)} = \boldU_{ij}^{(m)}$
- for ordinal views, $\boldX_{ij}^{(m)} = l$ if and only if $g_{l-1}^m < \boldU_{ij}^{(m)} < g_l^m$ for some cutpoints $g_0^m < ... < g_{L_m}^m$

And the labels $y_i$ are generated $y_i = \argmax_c{z_{ic}}$, where $z_{ic} \sim \N(\boldV_i \boldbeta_c, 1)$ for classes $c$ and $\boldbeta_c \in \R^R$. 

This is illustrated in the following schema (non-zero elements of $\boldW$ are shaded). 

![](figures/schema1.png)

# Testing MLFS method on simulated data

Generate two views, both with 10 features. Altogether 200 data points (100 for training, 100 for testing), number of classes $C=2$. 

We will consider three scenarios 

- Two Gaussian views
- One gaussian, one ordinal (with 3 levels)
- Both ordinal views (both with 3 levels)

The latent data was generated as follows


That is, data was generated as follows:

* $\boldV_i \sim \N(0, \boldI)$ with dimensionality $R=6$
* sparsity in $\boldW$ was introduced as shown in the figure, nonzero elements of $\boldW^{(m)}$ were drawn from $N(0, 1)$


Performance ("prediction accuracy") reported as the proportion of correct classifications. 

### Sanity check

Perfect scenario (data generated exactly according to the model). 

![](experiment1_files/figure-html/unnamed-chunk-2-1.png) 

Conclusion:

- I guess, there is no evident bug
- learning from ordinal features is less efficient than from Gaussian ones (expected, I guess)

### How fast does variational Bayes converge?

Explore the lower bound (ELBO) over iterations. 

Also, the authors have noted that the original likelihood remains unchanged for rotations Q,  
\[
\boldU_i^{(m)} \sim \N(\boldV_i \boldW^{(m)}, \gamma^{-1}I) = \N(\boldV_i \boldQ^{-1} \boldQ \boldW^{(m)}, \gamma^{-1}I)
\]
and they have suggested to introduce a rotation Q to the variational family, and optimise w.r.t. Q. Lets explore the effect of this (do it with and without rotation).  

![](experiment1_files/figure-html/unnamed-chunk-3-1.png) 

Conclusions: 

1. It seems that about 20 iterations is already enough in these cases, and that convergence (to a local maximum?) is fast. 
2. Rotation is beneficial. 
    - without optimising w.r.t Q, we reach some local extremum and cannot escape it the lower bound is clearly not maximised (cannot escape the local maximum?)
    - it also helps to introduce sparsity to W

### What if we increase the amount of noise

Lets increase the noise in generating $\boldU$, i.e. explore behaviour w.r.t. $\gamma$ in equation $\boldU_i^{(m)} \sim \N(\boldV_i \boldW^{(m)}, \frac{1}{\gamma_m} \boldI)$. 

![](experiment1_files/figure-html/unnamed-chunk-4-1.png) 








### Can the model (ARD prior) handle large number of irrelevant features?

Each view has (10 + irrelevant) features now

![](experiment1_files/figure-html/unnamed-chunk-7-1.png) 

### What if the dimensionality of latent space is misspecified?

In the generated data, the underlying latent space was 6-dimensional. When fitting the model, lets consider misspecified values $\{2, 4 < 6 < 10, 20\}$. 

![](experiment1_files/figure-html/unnamed-chunk-8-1.png) 

When this dimensionality is underestimated, it is expected that we may lose in performance (cannot capture so much information). When it is overestimated, it seems to do just fine (reassuring, so in practice "pick R large enough" should do the job). 

### What if there are multiple correlated features

To generate a scenario where there are multiple correlated features, for example

1. One way would be to generate $\boldU$ from $\boldV$ and $\boldW$ as usual, and then modify $\boldU$ by taking its columns and adding new correlated ones
2. Another way would be to create correlated columns in $\boldW$ (i.e. deviate from the assumption that the elements within the row of $\boldW$ are uncorrelated)

Lets start with approach 2. Having constructed original $\boldW^{(m)}$, add $\{1, 5, 10\}$ correlated copies of its columns (correlation with original feature around 0.8). 

![](experiment1_files/figure-html/unnamed-chunk-9-1.png) 

Everything seems OK. When trying out the other approach, similar behaviour was seen. 

Minor comment: Sometimes got an error $\boldQ \boldQ^T$ being not invertible. In such cases I proceeded without optimising w.r.t. rotations any more. 

### What if the Gaussianity assumption is violated

In $\boldU_i^{(m)} \sim \N(\boldV_i \boldW^{(m)}, \frac{1}{\gamma_m} \boldI)$, we can replace the Gaussian distribution with Student t distribution (df=1) with heavier tails. Note that if $\gamma$ is small, the effect of this is also small. In these experiments $\gamma=1$.  

In principle, we could also ask a similar question about Gaussianity in $\boldV$ or $\boldW$, but currently skip this. 

![](experiment1_files/figure-html/unnamed-chunk-10-1.png) 

Quite a large drop in performance. 
